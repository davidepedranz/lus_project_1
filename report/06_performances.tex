\section{Performances}
\label{sec:performances}

\subsection{Baseline}
We use the first model with a unigram and no smoothing a a baseline to evaluate the performances of the different n-gram orders and smoothing methods.
The model with these parameters assign to each word the concept with the highest frequency in the training set for the specific word.
The baseline model reached a F1 of $57.13\%$, as showed in \cref{tab:v1-ngrams}. 

\begin{table}[t!]
	\centering
    \begin{tabular}{ l l l l l }
    	\toprule
    		n & smoothing & prec. & recall & F1 \\
    	\midrule
            \input{table/v1-ngrams}
    	\bottomrule
	\end{tabular}
    \caption{Comparison of the performances of different n-gram orders for model 1.}
	\label{tab:v1-ngrams}
\end{table}

\subsection{n-gram Order}
The model without smoothing has the best performances for bigrams and 5-grams, reaching respectively a F1 of $76.21\%$  and $76.05\%$.
The smoothing helps to get slightly better performances, but does not change the ranking of the n-gram orders.
All models with n-gram order greater then $1$ perform significantly better than the baseline, with an increase in the F1 score of about $20\%$.

\begin{table}[t!]
	\centering
    \begin{tabular}{ l l l l l }
    	\toprule
    		n & smoothing & prec. & recall & F1 \\
    	\midrule
            \input{table/v1-smoothing}
    	\bottomrule
	\end{tabular}
    \caption{Comparison of the performances of different smoothing methods for model 1.}
	\label{tab:v1-smoothing}
\end{table}

\subsection{Smoothing Methods}
For both bigrams and 5-grams, the best smoothing method is Witten-Bell, followed by Absolute and Kneser-Ney.
\cref{tab:v1-smoothing} shows a detailed comparison of all smoothing methods.

\subsection{Features}
The model can be trained with a different feature rather than the word.
\cref{tab:v1-features} compares performances of the 5-grams model with Witten-Bell smoothing for different input features.
The best option is to use the words.
Words stems have slightly worse performances than words.
\ac{POS} have terrible performances, with a F1 score of only about $20\%$.

\begin{table}
	\centering
    \begin{tabular}{ l l l l l }
    	\toprule
    		feature & n & prec. & recall & F1 \\
    	\midrule
            \input{table/v1-features}
    	\bottomrule
	\end{tabular}
    \caption{Comparison of the performances of different input features for model 1, using Witten-Bell smoothing.}
	\label{tab:v1-features}
\end{table}

\subsection{Improved model}
The improved model described in \cref{sec:improvements} achieves significantly better performances than the original model, with a F1 score around $82\%$.
The best meta parameters for the model are 4-ngram language model with Kneser-Ney smoothing, with a F1 value of $82.74\%$.
Similarly to the original model, the additional feature result in worse performances.
Opposite to the previous case, the best smoothing methods seems to be Kneser-Ney.
More detailed results are showed in \cref{tab:v2}.

\begin{table}
	\centering
    \begin{tabular}{ l l l l l }
    	\toprule
    		n & smoothing & prec. & recall & F1 \\
    	\midrule
            \input{table/v2}
    	\bottomrule
	\end{tabular}
    \caption{Comparison of the performances of different parameters for model 2, using words as input.}
	\label{tab:v2}
\end{table}
