\section{Model}
\label{sec:model}

The model computes the sequence of tags $t_1, ..., t_n$ (also denoted as $t_1^n$) with the highest probability given the observed sequence of words 
$w_1, ..., w_n$:
\begin{equation}
    t_1, ..., t_n = \argmax_{t_1, ..., t_n} P(t_1^n | w_1^n).
    \label{eq:model_original}
\end{equation}

Applying the Bayes' Law, we obtain:
\begin{equation*}
    t_1, ..., t_n = \argmax_{t_1, ..., t_n} \frac{P(w_1^n | t_1^n) P(t_1^n)}{P(w_1^n)};
\end{equation*}

since the denominator does not depend on $t_1, ..., t_n$, we can drop it:
\begin{equation}
    t_1, ..., t_n = \argmax_{t_1, ..., t_n} P(w_1^n | t_1^n) P(t_1^n).
    \label{eq:model_compact}
\end{equation}

We assume that the probability of a word only depends on its own tag, and not on the tags of the other words in sentence.
Thus, we have that:
\begin{equation}
    P(w_1^n | t_1^n) \approx \prod_{i=1}^n P(w_i | t_i).
    \label{eq:model_first_term}
\end{equation}

Each $P(w_i | t_i)$ can be easily computed from the training data using maximum likelihood as:
\begin{equation*}
    P(w_i | t_i) = \frac{C(w_i, t_i)}{C(t_i)},
\end{equation*}
where $C(t_i, w_i)$ denotes the number of times that word $w_i$ has tag $t_i$ 
and $C(t_i)$ denotes the frequency of tag $t_i$.

The term $P(t_1^n)$ represent the probability of a tag given all its predecessor in the sentence.
To compute the term, we introduce a Markov assumption: the probability of a tag in the sentence
depends only on the $m$ previous tags:
\begin{equation}
    P(t_1, ..., t_n) = \prod_{i=m}^n P(t_i | t_{i-m}, ... t_{i-1}),
    \label{eq:model_second_term}
\end{equation}
where $t_0$ denotes the beginning of the sentence.
A Markov assumption of order $m$ corresponds to a $m+1$ n-gram language model.

Each term $P(t_i | t_{i-1})$ can be computed using maximum likelihood as follows:
\begin{equation*}
    P(t_i | t_{i-m}, ..., t_{i-1}) = \frac{C(t_{i-m}, ..., t_{i})}{C(t_{i-m}, ..., t_{i-1})}.
\end{equation*}

Using \cref{eq:model_compact}, (\ref{eq:model_first_term}) and (\ref{eq:model_second_term}), we can rewrite the model in \cref{eq:model_original} as:
\begin{equation*}
    t_1^n = \argmax_{t_1, ..., t_n} \prod_{i=1}^n P(w_i | t_i) \prod_{i=m}^n P(t_i | t_{i-m}^{i-1}).
\end{equation*}
