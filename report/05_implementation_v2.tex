\section{Language Model Improvement}

The model presented in \cref{sec:implementation} does not exploit the information provided by the words that are tagged as \texttt{O}.
In fact, all the information is lost with the transducer $\lambda_{word2concept}$.
We can preserve this information by including the word itself in the \texttt{O} tag, so that we can build a more accurate language model.

This idea is implemented in \texttt{model/v2.sh}.

Before building the lexicon, the training data is modified to replace the \texttt{O} tags with \texttt{O-word}.
The lexicon, $\lambda_{word2concept}$ and $\lambda_{concept\_lm}$ are build using the modified version of the training data.
Finally, a new transducer $\lambda_{concept2iob}$ is added to translate the \texttt{O-word} tags back to \texttt{O}.
This transducer has a single state $0$, a self transition $\texttt{O-word} \rightarrow \texttt{O}$ for each word in the lexicon and a two self transition $\texttt{B-concept} \rightarrow \texttt{B}$ and $\texttt{I-concept} \rightarrow \texttt{I}$ for each concept.
All transition have cost $0$.
For each possible input token, there is only one transition available.
